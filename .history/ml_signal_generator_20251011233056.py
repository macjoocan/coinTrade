# ml_signal_generator.py

import numpy as np
import pandas as pd
import pyupbit
import pickle
import logging
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import warnings

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

class MLSignalGenerator:
    """ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì§„ì… ì‹ í˜¸ ìƒì„±ê¸°"""
    
    def __init__(self, model_type='random_forest'):
        self.model_type = model_type
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = []
        self.is_trained = False
        
        # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        self.model_file = f'ml_model_{model_type}.pkl'
        self.scaler_file = 'ml_scaler.pkl'
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„°
        self.lookback_hours = 168  # 1ì£¼ì¼
        self.prediction_horizon = 6  # 6ì‹œê°„ í›„ ì˜ˆì¸¡
        self.min_profit_threshold = 0.015  # 1.5% ì´ìƒì„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
        
        # ëª¨ë¸ ë¡œë“œ ì‹œë„
        self._load_model()
    
    def train_model(self, symbols, retrain=False):
        """ëª¨ë¸ í•™ìŠµ"""
        
        if self.is_trained and not retrain:
            logger.info("ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        logger.info("="*60)
        logger.info("ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        logger.info("="*60)
        
        all_features = []
        all_labels = []
        
        # ê° ì‹¬ë³¼ë³„ ë°ì´í„° ìˆ˜ì§‘
        for symbol in symbols:
            logger.info(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘: {symbol}")
            
            features, labels = self._prepare_training_data(symbol)
            
            if features is not None and len(features) > 0:
                all_features.append(features)
                all_labels.append(labels)
                logger.info(f"  âœ… {symbol}: {len(features)}ê°œ ìƒ˜í”Œ ìˆ˜ì§‘")
            else:
                logger.warning(f"  âš ï¸ {symbol}: ë°ì´í„° ë¶€ì¡±")
        
        if not all_features:
            logger.error("í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return False
        
        # ë°ì´í„° ê²°í•©
        X = pd.concat(all_features, ignore_index=True)
        y = pd.concat(all_labels, ignore_index=True)
        
        logger.info(f"\nì´ í•™ìŠµ ë°ì´í„°: {len(X)}ê°œ")
        logger.info(f"ê¸ì • ìƒ˜í”Œ: {y.sum()}ê°œ ({y.mean():.1%})")
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # ëª¨ë¸ í•™ìŠµ
        logger.info("\nëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        if self.model_type == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=20,
                min_samples_leaf=10,
                random_state=42,
                n_jobs=-1
            )
        else:  # gradient_boosting
            self.model = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42
            )
        
        self.model.fit(X_train_scaled, y_train)
        
        # í‰ê°€
        train_score = self.model.score(X_train_scaled, y_train)
        test_score = self.model.score(X_test_scaled, y_test)
        
        logger.info(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
        logger.info(f"  í•™ìŠµ ì •í™•ë„: {train_score:.1%}")
        logger.info(f"  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_score:.1%}")
        
        # íŠ¹ì„± ì¤‘ìš”ë„
        if hasattr(self.model, 'feature_importances_'):
            importance = pd.DataFrame({
                'feature': X.columns,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            logger.info(f"\nì£¼ìš” íŠ¹ì„± (Top 5):")
            for idx, row in importance.head(5).iterrows():
                logger.info(f"  {row['feature']}: {row['importance']:.3f}")
        
        self.feature_names = list(X.columns)
        self.is_trained = True
        
        # ëª¨ë¸ ì €ì¥
        self._save_model()
        
        logger.info("="*60)
        return True
    
    def _prepare_training_data(self, symbol):
        """í•™ìŠµ ë°ì´í„° ì¤€ë¹„"""
        try:
            ticker = f"KRW-{symbol}"
            
            # ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ (ìµœëŒ€í•œ ë§ì´)
            df = pyupbit.get_ohlcv(ticker, interval="minute60", count=2000)
            
            if df is None or len(df) < 200:
                return None, None
            
            # íŠ¹ì„± ìƒì„±
            features_df = self._create_features(df)
            
            # ë ˆì´ë¸” ìƒì„± (ë¯¸ë˜ ìˆ˜ìµë¥ )
            future_returns = df['close'].shift(-self.prediction_horizon) / df['close'] - 1
            labels = (future_returns > self.min_profit_threshold).astype(int)
            
            # NaN ì œê±°
            valid_idx = ~(features_df.isna().any(axis=1) | labels.isna())
            
            features_df = features_df[valid_idx]
            labels = labels[valid_idx]
            
            return features_df, labels
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨ {symbol}: {e}")
            return None, None
    
    def _create_features(self, df):
        """íŠ¹ì„± ìƒì„±"""
        features = pd.DataFrame(index=df.index)
        
        # 1. ê°€ê²© íŠ¹ì„±
        features['returns_1h'] = df['close'].pct_change(1)
        features['returns_4h'] = df['close'].pct_change(4)
        features['returns_24h'] = df['close'].pct_change(24)
        
        # 2. ì´ë™í‰ê· 
        for period in [10, 20, 50]:
            features[f'sma_{period}'] = df['close'].rolling(period).mean()
            features[f'price_to_sma_{period}'] = df['close'] / features[f'sma_{period}']
        
        # 3. RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss
        features['rsi'] = 100 - (100 / (1 + rs))
        
        # 4. MACD
        ema_12 = df['close'].ewm(span=12).mean()
        ema_26 = df['close'].ewm(span=26).mean()
        features['macd'] = ema_12 - ema_26
        features['macd_signal'] = features['macd'].ewm(span=9).mean()
        features['macd_histogram'] = features['macd'] - features['macd_signal']
        
        # 5. ë³¼ë¦°ì € ë°´ë“œ
        sma_20 = df['close'].rolling(20).mean()
        std_20 = df['close'].rolling(20).std()
        features['bb_upper'] = sma_20 + (std_20 * 2)
        features['bb_lower'] = sma_20 - (std_20 * 2)
        features['bb_position'] = (df['close'] - features['bb_lower']) / (features['bb_upper'] - features['bb_lower'])
        
        # 6. ë³€ë™ì„±
        features['volatility'] = df['close'].pct_change().rolling(20).std()
        features['atr'] = self._calculate_atr(df)
        
        # 7. ë³¼ë¥¨ íŠ¹ì„±
        features['volume_sma'] = df['volume'].rolling(20).mean()
        features['volume_ratio'] = df['volume'] / features['volume_sma']
        features['volume_change'] = df['volume'].pct_change(1)
        
        # 8. ëª¨ë©˜í…€
        features['momentum_10'] = df['close'] / df['close'].shift(10) - 1
        features['momentum_20'] = df['close'] / df['close'].shift(20) - 1
        
        # 9. ìº”ë“¤ íŒ¨í„´ (ê°„ë‹¨í•œ ë²„ì „)
        features['candle_body'] = (df['close'] - df['open']).abs() / df['open']
        features['candle_upper_shadow'] = (df['high'] - df[['open', 'close']].max(axis=1)) / df['open']
        features['candle_lower_shadow'] = (df[['open', 'close']].min(axis=1) - df['low']) / df['open']
        
        # 10. ì‹œê°„ íŠ¹ì„±
        features['hour'] = df.index.hour
        features['day_of_week'] = df.index.dayofweek
        
        return features
    
    def _calculate_atr(self, df, period=14):
        """ATR ê³„ì‚°"""
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = np.max(ranges, axis=1)
        atr = true_range.rolling(period).mean()
        
        return atr / df['close']  # ì •ê·œí™”
    
    def predict(self, symbol):
        """ì˜ˆì¸¡ ì‹¤í–‰"""
        
        if not self.is_trained:
            logger.warning("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            ticker = f"KRW-{symbol}"
            
            # ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            df = pyupbit.get_ohlcv(ticker, interval="minute60", count=200)
            
            if df is None or len(df) < 100:
                return None
            
            # íŠ¹ì„± ìƒì„±
            features_df = self._create_features(df)
            
            # ìµœì‹  ë°ì´í„°ë§Œ ì‚¬ìš©
            latest_features = features_df.iloc[-1:][self.feature_names]
            
            # NaN ì²´í¬
            if latest_features.isna().any().any():
                logger.warning(f"{symbol}: íŠ¹ì„±ì— NaN ê°’ ì¡´ì¬")
                return None
            
            # ìŠ¤ì¼€ì¼ë§
            features_scaled = self.scaler.transform(latest_features)
            
            # ì˜ˆì¸¡
            probability = self.model.predict_proba(features_scaled)[0]
            prediction = self.model.predict(features_scaled)[0]
            
            return {
                'symbol': symbol,
                'prediction': bool(prediction),
                'buy_probability': float(probability[1]),
                'confidence': float(max(probability)),
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨ {symbol}: {e}")
            return None
    
    def get_signal(self, symbol, confidence_threshold=0.65):
        """ê±°ë˜ ì‹ í˜¸ ìƒì„±"""
        
        prediction = self.predict(symbol)
        
        if not prediction:
            return False, "ì˜ˆì¸¡ ë¶ˆê°€"
        
        if prediction['prediction'] and prediction['buy_probability'] >= confidence_threshold:
            return True, (f"ML ë§¤ìˆ˜ ì‹ í˜¸ (í™•ë¥ : {prediction['buy_probability']:.1%}, "
                         f"ì‹ ë¢°ë„: {prediction['confidence']:.1%})")
        
        return False, f"ML ì‹ í˜¸ ì•½í•¨ (í™•ë¥ : {prediction['buy_probability']:.1%})"
    
    def _save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        try:
            with open(self.model_file, 'wb') as f:
                pickle.dump({
                    'model': self.model,
                    'feature_names': self.feature_names,
                    'model_type': self.model_type
                }, f)
            
            with open(self.scaler_file, 'wb') as f:
                pickle.dump(self.scaler, f)
            
            logger.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.model_file}")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            with open(self.model_file, 'rb') as f:
                data = pickle.load(f)
                self.model = data['model']
                self.feature_names = data['feature_names']
                self.model_type = data['model_type']
            
            with open(self.scaler_file, 'rb') as f:
                self.scaler = pickle.load(f)
            
            self.is_trained = True
            logger.info(f"âœ… ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.model_file}")
            
        except FileNotFoundError:
            logger.info("ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def evaluate_recent_performance(self, symbols, days=7):
        """ìµœê·¼ ì„±ëŠ¥ í‰ê°€"""
        
        if not self.is_trained:
            logger.warning("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š ìµœê·¼ {days}ì¼ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
        logger.info(f"{'='*60}")
        
        total_signals = 0
        successful_signals = 0
        
        for symbol in symbols:
            ticker = f"KRW-{symbol}"
            
            try:
                # ìµœê·¼ ë°ì´í„°
                df = pyupbit.get_ohlcv(ticker, interval="minute60", count=24*days)
                
                if df is None or len(df) < 100:
                    continue
                
                # íŠ¹ì„± ìƒì„±
                features_df = self._create_features(df)
                
                # ê° ì‹œì ì—ì„œ ì˜ˆì¸¡
                for i in range(len(df) - self.prediction_horizon - 50):
                    current_features = features_df.iloc[i:i+1][self.feature_names]
                    
                    if current_features.isna().any().any():
                        continue
                    
                    # ì˜ˆì¸¡
                    features_scaled = self.scaler.transform(current_features)
                    probability = self.model.predict_proba(features_scaled)[0][1]
                    
                    if probability >= 0.65:  # ì‹ í˜¸ ë°œìƒ
                        total_signals += 1
                        
                        # ì‹¤ì œ ê²°ê³¼ í™•ì¸
                        future_price = df.iloc[i + self.prediction_horizon]['close']
                        current_price = df.iloc[i]['close']
                        actual_return = (future_price - current_price) / current_price
                        
                        if actual_return > self.min_profit_threshold:
                            successful_signals += 1
                
            except Exception as e:
                logger.error(f"{symbol} í‰ê°€ ì‹¤íŒ¨: {e}")
                continue
        
        if total_signals > 0:
            success_rate = successful_signals / total_signals
            logger.info(f"\nì´ ì‹ í˜¸: {total_signals}ê°œ")
            logger.info(f"ì„±ê³µ: {successful_signals}ê°œ")
            logger.info(f"ì„±ê³µë¥ : {success_rate:.1%}")
        else:
            logger.info("\ní‰ê°€ ê¸°ê°„ ë™ì•ˆ ì‹ í˜¸ ì—†ìŒ")
        
        logger.info(f"{'='*60}\n")